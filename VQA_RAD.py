"""
_*_CODING:UTF-8_*_
@Author: Yu Hou
@File: VQA_RAD.py
@Time: 9/28/25; 10:57â€¯AM
"""
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Evaluate GPT-5 and GPT-4o on VQA-RAD (Radiology VQA, multi-modal).
- Uses CloudFront images: https://d2rfm59k9u0hrr.cloudfront.net/medpix/img/full/{image_name}
- Closed-ended Yes/No subset by default (objective auto-eval).
- Supports 0/1/5-shot with multi-modal exemplars.
Outputs:
  - results_vqarad_<model>_<k_shot>.csv
  - summary_vqarad_k_shot.json
"""

import os, re, json, time, random
from dataclasses import dataclass, asdict
from typing import List, Tuple, Dict, Any, Optional

import pandas as pd
from tqdm import tqdm
from openai import OpenAI

# ============== Config ==============
DATA_PATHS = [
    "VQA_RAD Dataset Public.json",
    os.path.join("/Users/hou00127/Google/Works/2025/Benchmarks/version_2/VQA_RAD", "VQA_RAD Dataset Public.json"),
]
CLOUDFRONT_PREFIX = "https://d2rfm59k9u0hrr.cloudfront.net/medpix/img/full/"

MODELS = [
    "gpt-5",
    "gpt-4o",
]

# Pricing (USD per 1K tokens) â€” ä¸ä½ çš„ MedQA è„šæœ¬ä¸€è‡´
PRICING_PER_1K = {
    "gpt-5": {"input": 0.005, "output": 0.015},  # $5/M in, $15/M out
    "gpt-4o": {"input": 0.0025, "output": 0.010},  # $2.5/M in, $10/M out
}

# å…¥å£å‚æ•°ï¼šè®¾ç½® None è·‘å…¨é‡ï¼›æˆ–è®¾ç½®ä¸ºæ•´æ•°åšå°æ ·æœ¬å¿«é€Ÿæµ‹è¯•
MAX_SAMPLES = None  # e.g., 50
RANDOM_SEED = 42
TIMEOUT_S = 60

# åªè¯„ä¼° Yes/No å­é›†ï¼ˆæ›´æ˜“å®¢è§‚è¯„ä¼°ï¼‰
ONLY_YESNO_CLOSED = True

# ï¼ˆå¯é€‰ï¼‰æ˜¯å¦è¯„ä¼° open-endedï¼ˆéœ€è¦äºŒæ¬¡è°ƒç”¨ LLM ä½œä¸º Judgeï¼›é»˜è®¤å…³é—­ï¼‰
EVAL_OPEN_ENDED = False
JUDGE_MODEL = "gpt-5"

# Refusal æ£€æµ‹ï¼ˆä¸ MedQA è„šæœ¬åŒé£æ ¼ï¼‰
REFUSAL_PATTERNS = [
    r"i\s+cannot\s+answer",
    r"i\s+can't\s+answer",
    r"unable\s+to\s+answer",
    r"as\s+an\s+ai",
    r"i\s+apologize",
    r"sorry[, ]\s+i\s+cannot",
]

# ============== Client ==============
OPENAI_API_KEY = ""
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

client = OpenAI(api_key=OPENAI_API_KEY)


# ============== Utils ==============
def estimate_cost_usd(model: str, in_tokens: int, out_tokens: int) -> float:
    p = PRICING_PER_1K.get(model, {"input": 0.0, "output": 0.0})
    return (in_tokens / 1000.0) * p["input"] + (out_tokens / 1000.0) * p["output"]


def is_refusal(text: str) -> bool:
    if not text:
        return False
    t = text.strip().lower()
    return any(re.search(p, t) for p in REFUSAL_PATTERNS)


def read_vqarad(path_candidates: List[str]) -> List[Dict[str, Any]]:
    for p in path_candidates:
        if os.path.exists(p):
            with open(p, "r", encoding="utf-8") as f:
                return json.load(f)
    raise FileNotFoundError(f"Cannot find dataset in: {path_candidates}")


def normalize_yesno(s: str) -> str:
    if s is None:
        return ""
    x = s.strip().lower()
    if x in ("yes", "y", "true"):
        return "Yes"
    if x in ("no", "n", "false"):
        return "No"
    return ""


def make_image_url(image_name: str) -> str:
    return f"{CLOUDFRONT_PREFIX}{image_name}"


# æå– Final Answer: Yes/No
def extract_final_answer(text: str) -> str:
    """
    æå– `Final Answer: ...` çš„å†…å®¹ã€‚
    - æ”¯æŒ Yes/No (closed)
    - æ”¯æŒ word/phrase (open)
    """
    if not text:
        return ""
    m = re.search(r"Final\s*Answer\s*[:ï¼š]?\s*(.+)", text, flags=re.I)
    if m:
        return m.group(1).strip()
    return ""


# ä¸ MedQA ä¸€è‡´çš„é”™è¯¯åˆ†ç±»æ¡†æ¶ï¼ˆæ ¹æ® Yes/Noï¼‰
def classify_errors(pred_choice: str, gold: str, valid: Tuple[str, str], model_text: str, is_correct: int):
    """
    Returns (missing, inconsistent, hallucinated) as 0/1 (mutually exclusive).
    Priority: Missing > Invalid Label (as inconsistent) > Hallucinated > Inconsistent
    """
    if is_correct == 1:
        return 0, 0, 0
    # 1) Missing
    if not pred_choice:
        return 1, 0, 0
    # 2) Invalid -> inconsistent
    if pred_choice not in valid:
        return 0, 1, 0
    # 3) ï¼ˆä¿ç•™ä½ï¼‰Hallucinatedï¼šè¿™é‡Œå¯¹ Yes/No éš¾å®šä¹‰ï¼Œé»˜è®¤ 0
    # å¯æŒ‰éœ€æ‰©å±•ï¼šä¾‹å¦‚æ£€æµ‹äº’ç›¸çŸ›ç›¾çš„åŒé‡ç­”æ¡ˆç­‰
    return 0, 0, 0


@dataclass
class ItemLog:
    qid: str
    answer_type: str
    model: str
    k_shot: int
    latency_ms: float
    tokens_in: int
    tokens_out: int
    cost_usd: float
    gold: str
    pred: str
    is_correct: int
    refusal: int
    err_missing: int
    err_inconsistent: int
    err_hallucinated: int
    raw_output: str


# ============== Data Loader (to DF like MedQA) ==============
def load_vqarad(max_samples: Optional[int] = None, seed: int = 42,
                only_yesno_closed: bool = True, include_open: bool = False) -> pd.DataFrame:
    random.seed(seed)
    data = read_vqarad(DATA_PATHS)

    rows = []
    for ex in data:
        qid = str(ex.get("qid"))
        q = str(ex.get("question") or "").strip()
        image_name = str(ex.get("image_name") or "").strip()
        answer_type = str(ex.get("answer_type") or "").strip().upper()
        gold_raw = str(ex.get("answer") or "").strip()

        if answer_type == "CLOSED":
            if only_yesno_closed:
                gold = normalize_yesno(gold_raw)
                if gold not in ("Yes", "No"):
                    continue
            else:
                gold = gold_raw
        elif answer_type == "OPEN":
            if not include_open:
                continue
            gold = gold_raw
        else:
            continue

        if not image_name or not q:
            continue

        rows.append({
            "id": qid,
            "question": q,
            "image_url": make_image_url(image_name),
            "gold": gold,
            "answer_type": answer_type,
            "image_name": image_name
        })

    df = pd.DataFrame(rows)
    if max_samples is not None and len(df) > max_samples:
        df = df.sample(n=max_samples, random_state=seed).reset_index(drop=True)
    return df


# ============== Prompt (0/1/5-shot, multi-modal) ==============
def build_prompt_mm(question: str, k_shot: int, examples: List[Dict[str, Any]], answer_type: str = "CLOSED"):
    system_prompt = "You are a board-certified radiologist. Answer accurately based only on the image."
    parts: List[Dict[str, Any]] = []

    # few-shot examples (åªç”¨ CLOSED åš few-shot ç¤ºä¾‹)
    if k_shot > 0 and examples:
        for ex in examples[:k_shot]:
            ex_q = ex["question"]
            ex_gold = ex["gold"]  # Yes/No
            ex_img = ex["image_url"]
            parts.append({"type": "text", "text": f"Example Question: {ex_q}"})
            parts.append({"type": "image_url", "image_url": {"url": ex_img}})
            parts.append({"type": "text", "text":
                "Answer briefly. Final line must be exactly:\n"
                "Final Answer: Yes\nor\nFinal Answer: No"
                          })
            parts.append({"type": "text", "text": f"Final Answer: {ex_gold}"})

    # main question
    if answer_type == "CLOSED":
        instruction = (
            f"Question: {question}\n\n"
            "First give a very brief reasoning (<= 30 words).\n"
            "Then on the final line, output strictly one of:\n"
            "Final Answer: Yes\n"
            "or\n"
            "Final Answer: No"
        )
    else:  # OPEN
        instruction = (
            f"Question: {question}\n\n"
            "First give a very brief reasoning (<= 30 words).\n"
            "Then on the final line, output strictly:\n"
            "Final Answer: <one word or short phrase>"
        )

    parts.append({"type": "text", "text": instruction})
    return system_prompt, parts


# ============== OpenAI call (multi-modal) ==============
def call_gpt_vision(system_prompt: str, user_parts: List[Dict[str, Any]], model: str) -> Dict[str, Any]:
    start = time.time()
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_parts},
            ],
            timeout=TIMEOUT_S,
        )
        latency_ms = (time.time() - start) * 1000.0
        text = response.choices[0].message.content if response.choices else ""
        usage = getattr(response, "usage", None) or {}
        in_tok = getattr(usage, "prompt_tokens", 0)
        out_tok = getattr(usage, "completion_tokens", 0)
        return {
            "ok": True,
            "text": text or "",
            "latency_ms": latency_ms,
            "input_tokens": in_tok or 0,
            "output_tokens": out_tok or 0,
            "error": None,
        }
    except Exception as e:
        return {
            "ok": False, "text": "", "latency_ms": (time.time() - start) * 1000.0,
            "input_tokens": 0, "output_tokens": 0, "error": str(e),
        }


# ============== Evaluation ==============
def sample_examples(df: pd.DataFrame, k_shot: int, seed: int = RANDOM_SEED) -> List[Dict[str, Any]]:
    if k_shot <= 0 or df.empty:
        return []
    pool = df.sample(n=min(max(k_shot, 5), len(df)), random_state=seed)  # å¤šæŠ½ä¸€ç‚¹ï¼Œä¿è¯å……è¶³
    return pool.to_dict(orient="records")


def eval_model_on_vqarad(model: str, df: pd.DataFrame, k_shot: int = 0):
    logs: List[ItemLog] = []

    # few-shot ç¤ºä¾‹æ± ï¼ˆåªä» CLOSED é‡ŒæŠ½ï¼Œä¿è¯æ˜¯ Yes/No ç¤ºä¾‹ï¼‰
    examples = sample_examples(df[df["answer_type"] == "CLOSED"], k_shot=k_shot, seed=RANDOM_SEED)

    for _, row in tqdm(df.iterrows(), total=len(df), desc=f"Evaluating {model} (k={k_shot})"):
        sys_p, parts = build_prompt_mm(row["question"], k_shot=k_shot, examples=examples, answer_type=row["answer_type"])

        # å½“å‰é—®é¢˜çš„å›¾ç‰‡
        parts.append({"type": "image_url", "image_url": {"url": row["image_url"]}})

        # è°ƒç”¨æ¨¡å‹
        resp = call_gpt_vision(sys_p, parts, model=model)
        text = resp["text"]

        if row["answer_type"] == "CLOSED":
            pred_choice = extract_final_answer(text)
            is_correct = 1 if (pred_choice.capitalize() == row["gold"]) else 0
            refusal = 1 if is_refusal(text) else 0
            err_missing, err_inconsistent, err_hallucinated = classify_errors(
                pred_choice=pred_choice,
                gold=row["gold"],
                valid=("Yes", "No"),
                model_text=text,
                is_correct=is_correct,
            )

        else:  # OPEN
            pred_choice = extract_final_answer(text)
            refusal = 1 if is_refusal(pred_choice) else 0
            # ä¼˜å…ˆ exact match
            if pred_choice.lower() == row["gold"].lower():
                is_correct = 1
            else:
                # fallback: ç”¨ GPT-5 Judge åˆ¤å®šè¯­ä¹‰ç­‰ä»·
                try:
                    judge = judge_semantic_equivalence(row["gold"], pred_choice, model=JUDGE_MODEL)
                    is_correct = 1 if judge else 0
                except Exception as e:
                    is_correct = 0
            err_missing = err_inconsistent = err_hallucinated = 0

        cost = estimate_cost_usd(model, resp["input_tokens"], resp["output_tokens"])
        logs.append(ItemLog(
            qid=row["id"], answer_type=row["answer_type"], model=model, k_shot=k_shot,
            latency_ms=resp["latency_ms"],
            tokens_in=resp["input_tokens"], tokens_out=resp["output_tokens"],
            cost_usd=cost,
            gold=row["gold"], pred=pred_choice, is_correct=is_correct,
            refusal=refusal,
            err_missing=err_missing, err_inconsistent=err_inconsistent, err_hallucinated=err_hallucinated,
            raw_output=text,
        ))

    df_log = pd.DataFrame([asdict(x) for x in logs])

    if df_log.empty:
        summary = {
            "model": model,
            "k_shot": k_shot,
            "n": 0,
            "accuracy": None,
            "avg_tokens_in": None,
            "avg_tokens_out": None,
            "total_cost_usd": 0.0,
            "avg_latency_ms": None,
            "missing_rate": None,
            "inconsistent_rate": None,
            "hallucinated_rate": None,
            "refusal_rate": None,
        }
        return df_log, summary

    summary = {
        "model": model,
        "k_shot": k_shot,
        "n": len(df_log),
        "accuracy": round(df_log["is_correct"].mean(), 4),
        "avg_tokens_in": round(df_log["tokens_in"].mean(), 2),
        "avg_tokens_out": round(df_log["tokens_out"].mean(), 2),
        "total_cost_usd": round(df_log["cost_usd"].sum(), 4),
        "avg_latency_ms": round(df_log["latency_ms"].mean(), 2),
        "missing_rate": round(df_log["err_missing"].mean(), 4),
        "inconsistent_rate": round(df_log["err_inconsistent"].mean(), 4),
        "hallucinated_rate": round(df_log["err_hallucinated"].mean(), 4),
        "refusal_rate": round(df_log["refusal"].mean(), 4),
    }
    return df_log, summary


# ï¼ˆå¯é€‰ï¼‰Open-ended è¯„ä¼°ï¼šç”¨ GPT-5 ä½œä¸º Judgeï¼ˆé»˜è®¤å…³é—­ï¼‰
def judge_semantic_equivalence(gold: str, pred: str, model: str = JUDGE_MODEL) -> bool:
    """
    è®© LLM ä½œä¸ºè¯„å®¡ï¼Œåˆ¤æ–­ pred æ˜¯å¦ä¸ gold è¯­ä¹‰ç­‰ä»·ï¼ˆè¿”å› True/Falseï¼‰ã€‚
    æ³¨æ„ï¼šä¼šäº§ç”Ÿé¢å¤– API æˆæœ¬ï¼›é»˜è®¤ä¸å¯ç”¨ã€‚
    """
    sys_p = "You are an expert radiology QA judge."
    usr = (
        "Decide if the predicted answer is semantically equivalent to the gold answer.\n"
        "Answer strictly with one line: Final Answer: Yes or Final Answer: No\n\n"
        f"Gold: {gold}\n"
        f"Pred: {pred}\n"
    )
    r = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": sys_p}, {"role": "user", "content": usr}],
        timeout=TIMEOUT_S,
    )
    y = r.choices[0].message.content or ""
    lab = extract_final_answer(y)
    return lab == "Yes"


# ---------- Load JSON (robust for JSONL) ----------
def load_json_or_jsonl(path):
    with open(path, "r", encoding="utf-8") as f:
        text = f.read().strip()
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            return [json.loads(line) for line in text.splitlines() if line.strip()]


# ---------- Build mapping ----------
def build_qtype_map(json_path):
    data = load_json_or_jsonl(json_path)
    qtype_map = {}
    for item in data:
        qid = str(item.get("qid")).strip()
        qtype = item.get("question_type", "Other")
        qtype = ",".join([t.strip().upper() for t in qtype.split(",")]) if qtype else "OTHER"
        qtype_map[qid] = qtype
    print(f"[INFO] Loaded {len(qtype_map)} question_type mappings.")
    return qtype_map


def results_analysis(data_folder="/Users/hou00127/Google/Works/2025/Benchmarks/version_2/VQA_RAD/", model="gpt-5"):
    json_path = data_folder + "VQA_RAD Dataset Public.json"
    csv_path = data_folder + "results/results_vqarad_" + model + "_0.csv"
    output_csv = data_folder + "results/results_vqarad_with_type_" + model + ".csv"
    summary_json = data_folder + "results/summary_vqarad_by_type_" + model + ".json"

    # Load question type mapping
    qtype_map = build_qtype_map(json_path)

    # Load result CSV
    df = pd.read_csv(csv_path)
    id_col = next((c for c in df.columns if c.lower() in ["id", "qid", "question_id"]), None)
    if not id_col:
        raise ValueError("âŒ CSV must contain an 'id' or 'qid' column.")

    # Normalize qid and add question_type
    df["qid_norm"] = df[id_col].astype(str).str.strip()
    df["question_type"] = df["qid_norm"].map(lambda x: qtype_map.get(x, "OTHER"))

    # è¯†åˆ«é¢„æµ‹åˆ—ä¸çœŸå®åˆ—
    pred_col = next((c for c in df.columns if c.lower() in ["pred", "prediction", "model_output", "response", "pred_answer"]), None)
    gold_col = next((c for c in df.columns if c.lower() in ["gold", "answer", "label", "correct_answer"]), None)
    if not pred_col or not gold_col:
        raise ValueError("âŒ CSV must include columns for 'pred' and 'gold' answers.")

    # è®¡ç®—æ˜¯å¦æ­£ç¡®ï¼ˆå¿½ç•¥å¤§å°å†™ä¸ç©ºæ ¼ï¼‰
    df["is_correct"] = df.apply(
        lambda r: str(r[pred_col]).strip().lower() == str(r[gold_col]).strip().lower(), axis=1
    )

    # ä¿å­˜å¸¦ question_type çš„ CSV
    df.to_csv(output_csv, index=False)
    print(f"[âœ…] Saved merged file with question_type â†’ {output_csv}")

    # ---------- Accuracy summary ----------
    overall_acc = df["is_correct"].mean()
    type_stats = (
        df.groupby("question_type")
        .agg(n_total=("is_correct", "size"),
             n_correct=("is_correct", "sum"),
             accuracy=("is_correct", "mean"))
        .reset_index()
        .sort_values("accuracy", ascending=False)
    )

    # ---------- Save summary JSON ----------
    summary = {
        "model": "gpt-5",
        "n_total": int(len(df)),
        "overall_accuracy": round(overall_acc, 4),
        "type_summary": {
            row["question_type"]: {
                "n_total": int(row["n_total"]),
                "n_correct": int(row["n_correct"]),
                "accuracy": round(row["accuracy"], 4)
            }
            for _, row in type_stats.iterrows()
        }
    }

    with open(summary_json, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2)

    print(f"[âœ…] Saved summary JSON â†’ {summary_json}")
    print("\nğŸ“Š Per-type accuracy summary:")
    print(type_stats)


# ============== Main ==============
def main():
    # random.seed(RANDOM_SEED)
    # df = load_vqarad(max_samples=MAX_SAMPLES, seed=RANDOM_SEED, only_yesno_closed=ONLY_YESNO_CLOSED, include_open=EVAL_OPEN_ENDED)
    # summaries = []
    # for model in MODELS:
    #     for k_shot in [0, 1, 5]:
    #         df_log, summary = eval_model_on_vqarad(model, df, k_shot=k_shot)
    #         out_csv = f"results_vqarad_{model}_{k_shot}.csv"
    #         df_log.to_csv(out_csv, index=False)
    #         print(f"[Saved] {out_csv}")
    #         print(summary)
    #         summaries.append(summary)
    #
    # with open("summary_vqarad_k_shot.json", "w", encoding="utf-8") as f:
    #     json.dump(summaries, f, indent=2)
    # print("[Saved] summary_vqarad_k_shot.json")

    results_analysis(model="gpt-5")


if __name__ == "__main__":
    main()
